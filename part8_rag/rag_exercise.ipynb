{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 와 LLM 을 활용한 질의 응답 실습\n",
    "\n",
    " - LLM API 호출 시 RAG를 통해 연관 정보 추출 후 Prompt 에 추가하는 실습\n",
    " - RAG 추가 전후로 평가 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./res/rag_data.pkl', 'rb') as f:\n",
    "    rag_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = rag_data['questions'][:10]\n",
    "contexts = rag_data['contexts'][:10]\n",
    "answers = rag_data['answers'][:10]\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가\n",
    "- RAGAS의 Answer Correctedness 평가 지표 end to end\n",
    "  - 모델의 답변과 실제 정답 간의 일치율 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:09<00:00,  6.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from utils import call_openai, get_embeddings, cosine_similarity\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "    prompt = f\"\"\"You are an expert in Finance. Please answer to the question given below.\n",
    "    \n",
    "Question:\n",
    "{questions[i]}\n",
    "\"\"\"\n",
    "\n",
    "    prediction = call_openai(prompt, model='gpt-4o-2024-05-13')\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'set_run_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     14\u001b[0m data_samples \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: questions,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m: answers\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(data_samples)\n\u001b[1;32m---> 22\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43manswer_correctness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m score\n",
      "File \u001b[1;32md:\\myProjects\\ai-deep-learning\\part8_rag\\.venv\\Lib\\site-packages\\ragas\\_analytics.py:205\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m    204\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 205\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\myProjects\\ai-deep-learning\\part8_rag\\.venv\\Lib\\site-packages\\ragas\\evaluation.py:220\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size)\u001b[0m\n\u001b[0;32m    217\u001b[0m             reproducable_metrics\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# init all the models\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m executor \u001b[38;5;241m=\u001b[39m Executor(\n\u001b[0;32m    223\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m     keep_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    229\u001b[0m )\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Ragas Callbacks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# init the callbacks we need for various tasks\u001b[39;00m\n",
      "File \u001b[1;32md:\\myProjects\\ai-deep-learning\\part8_rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_correctness.py:198\u001b[0m, in \u001b[0;36mAnswerCorrectness.init\u001b[1;34m(self, run_config)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_config: RunConfig):\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_similarity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_similarity \u001b[38;5;241m=\u001b[39m AnswerSimilarity(embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings)\n",
      "File \u001b[1;32md:\\myProjects\\ai-deep-learning\\part8_rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py:230\u001b[0m, in \u001b[0;36mMetricWithLLM.init\u001b[1;34m(self, run_config)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no valid LLM provided (self.llm is None). Please initantiate a the metric with an LLM to run.\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_run_config\u001b[49m(run_config)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'set_run_config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from datasets import Dataset \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "\n",
    "load_dotenv()\n",
    "MY_OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(api_key=MY_OPENAI_KEY)\n",
    "\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': predictions,\n",
    "    'ground_truth': answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset, metrics=[answer_correctness], llm=llm)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글로벌 저금리 현상이 부각된 원인은 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글로벌 저금리 현상이 부각된 원인은 여러 가지가 복합적으로 작용한 결과입니다. 주요 원인들을 살펴보면 다음과 같습니다:\n",
      "\n",
      "1. **중앙은행의 통화정책**: 글로벌 금융위기 이후 많은 국가의 중앙은행들이 경제 회복을 촉진하기 위해 금리를 낮추고 양적 완화 정책을 시행했습니다. 이러한 정책들은 시장에 유동성을 공급하고 금리를 낮추는 효과를 가져왔습니다.\n",
      "\n",
      "2. **저성장 및 저물가**: 선진국을 중심으로 경제 성장률이 둔화되고 물가 상승률이 낮아지면서 중앙은행들은 금리를 낮게 유지할 필요성을 느꼈습니다. 저성장과 저물가는 금리를 낮추는 주요 요인 중 하나입니다.\n",
      "\n",
      "3. **인구 고령화**: 많은 선진국에서 인구 고령화가 진행되면서 소비보다는 저축이 증가하는 경향이 있습니다. 이는 자본 공급을 증가시키고 금리를 낮추는 요인으로 작용합니다.\n",
      "\n",
      "4. **글로벌 불확실성**: 무역 갈등, 지정학적 리스크, 코로나19 팬데믹 등 글로벌 불확실성이 증가하면서 안전자산에 대한 수요가 높아졌습니다. 이는 국채와 같은 안전자산의 금리를 낮추는 결과를 초래했습니다.\n",
      "\n",
      "5. **기술 발전**: 기술 발전으로 인해 생산성이 향상되고 비용이 절감되면서 물가 상승 압력이 줄어들었습니다. 이는 중앙은행들이 금리를 낮게 유지할 수 있는 환경을 조성했습니다.\n",
      "\n",
      "6. **부채 증가**: 많은 국가들이 높은 부채 수준을 유지하고 있으며, 이는 금리를 낮게 유지해야 하는 압력으로 작용합니다. 높은 금리는 부채 상환 부담을 증가시키기 때문에 정부와 기업들은 낮은 금리를 선호합니다.\n",
      "\n",
      "이러한 요인들이 복합적으로 작용하면서 글로벌 저금리 현상이 지속되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM + RAG\n",
    "- text-embedding-3-large 임베딩 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import call_openai, get_embeddings, cosine_similarity\n",
    "\n",
    "def retrieve_context(question, contexts):\n",
    "    question_embedding = get_embeddings([question], model='text-embedding-3-large')[0]\n",
    "    context_embeddings = get_embeddings(contexts, model='text-embedding-3-large')\n",
    "\n",
    "    similarities = [cosine_similarity(question_embedding, context_embedding) for context_embedding in context_embeddings]\n",
    "\n",
    "    most_relevant_index = np.argmax(similarities)\n",
    "    \n",
    "    return contexts[most_relevant_index]\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "    context = retrieve_context(questions[i], contexts[i])\n",
    "    prompt = f\"\"\"You are an expert in Finance. Please answer to the question given below. Use information given in Context appropriately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{questions[i]}\n",
    "\"\"\"\n",
    "    prediction = call_openai(prompt, model='gpt-4o-2024-05-13')\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_relevancy, context_recall, context_precision\n",
    "\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': predictions,\n",
    "    'ground_truth': answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset, metrics=[answer_correctness])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag env",
   "language": "python",
   "name": "ragenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
