{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API 비교\n",
    "  - 비용으로 1차적으로 추린 후에 2차적으로 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API\n",
    "  - 다른 LLM 라인업 대비 할상 좀 더 저렴한 가격에 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수에 GOOGLE_API_KEY 추가 필요\n",
    "# ~/.zshrc에 export GOOGLE_API_KEY='<API KEY>'\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"Hi!\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude API\n",
    "- OpenAI에 필적하는 성능\n",
    "  - 2024년 6월 기준 가장 좋은 성능의 모델 = Claude 3.5 Sonnet\n",
    "- 현재 기준 가장 비싼 모델인 Claude 3 Opus 보유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수에 ANTHROPIC_API_KEY 추가 필요\n",
    "# ~/.zshrc에 export ANTHROPIC_API_KEY='<API KEY>'\n",
    "import os\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ['ANTHROPIC_API_KEY']\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동일한 Prompt에 대해 Input / Output 계산\n",
    "- 비용 계산을 위해서는 Tokens 수를 알아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import get_eval_data\n",
    "\n",
    "conversations = get_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini Pro & Flash\n",
    "\n",
    "- 506 / 116 / 622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"아래 사용자 대화에 대해 3문장 내로 요약해주세요:\n",
    "\n",
    "{conversations[0]}\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-001')\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 호출해보기 전에 Prompt가 몇 토큰인 지 미리 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로는 여기에 token 1씩 더 추가됨\n",
    "model.count_tokens(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 Prompt에 대해 Gemini Pro와 Gemini Flash 둘 다 506 입력 토큰 -> 동일한 Tokenizer를 사용하는 것을 알 수 있음\n",
    "\n",
    "- 더 많은 단어를 익힌 (보통 더 큰) 모델의 경우 다른 Tokenizer를 써서 입력 토큰 수가 달라질 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"아래 사용자 대화에 대해 3문장 내로 요약해주세요:\n",
    "\n",
    "{conversations[0]}\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-pro-001')\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic Claude\n",
    "\n",
    "- 637 / 143 / 780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ['ANTHROPIC_API_KEY']\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "message.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "message.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "message.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI\n",
    "\n",
    "- 622 / 177 (GPT 3.5, GPT-4)\n",
    "- 441 / 76 (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "completion.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4-turbo-2024-04-09',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "completion.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-2024-05-13',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "completion.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Tokens 수 정리 (Prompt 고정)\n",
    "\n",
    "- Gemini 506\n",
    "- Claude 637\n",
    "- GPT 622\n",
    "- GPT-4o 441\n",
    "  - Output은 어느 정도 조정이 가능해서 Input만 비교\n",
    "\n",
    "- 한글 효율화 측면에서는 GPT-4o > Gemini > GPT-3.5/4 >= Claude 순\n",
    "  - 다른 Prompt에서는 조금씩 다를 수 있으나, 경향이 크게 달라지지 않을 것으로 예상\n",
    "\n",
    "#### 비용 계산\n",
    "- Input Tokens는 바로 위에 수치 사용하고, Output Tokens는 가장 많은 Claude를 200 Tokens로 두고 비례해서 계산\n",
    "  - ex. Gemini 159 Tokens\n",
    "- 호출 횟수 별 가격표\n",
    "  - https://docs.google.com/spreadsheets/d/1L4jfa8F7ifSbp779pInSRkT_JFnT57xsmJvOhvePOk4/edit?usp=sharing\n",
    "\n",
    "### 모델 별 가격 (1M Token, 2024.06)\n",
    "\n",
    "Claude 3 Opus / 75.00\n",
    "Claude 3 Sonnet / 15.00\n",
    "Claude 3 Haiku / 1.25\n",
    "Gemini Pro / 10.5 (일정 요청량까지 무료)\n",
    "Gemini Flash / 1.05 (일정 요청량까지 무료)\n",
    "GPT-4o / 15\n",
    "GPT-4 / 30\n",
    "GPT-3.5 / 1.5\n",
    "\n",
    "#### 1회 호출 비용\n",
    "\n",
    "- Claude 3 Opus $0.024 / 33.89원\n",
    "- Claude 3 Sonnet $0.004 / 6.78원\n",
    "- Claude 3 Haiku $0.0004 / 0.56원\n",
    "- Gemini Pro $0.003 / 4.75원\n",
    "- Gemini Flash $0.0003 / 0.47원\n",
    "- GPT-4o $0.004 / 5.91원\n",
    "- GPT-4 $0.012 / 16.67원\n",
    "- GPT-3.5 $0.0006 / 0.83원\n",
    "\n",
    "#### 10000회 호출 비용\n",
    "\n",
    "- Claude 3 Opus $245.55 / 338,859원\n",
    "- Claude 3 Sonnet $49.11 / 67,772원\n",
    "- Claude 3 Haiku $4.09 / 5,648원\n",
    "- Gemini Pro $34.39 / 47,460원\n",
    "- Gemini Flash $3.44 / 4,746원\n",
    "- GPT-4o $42.82 / 59,091원\n",
    "- GPT-4 $120.79 / 166,686원\n",
    "- GPT-3.5 $6.04 / 8,334원\n",
    "\n",
    "#### 100만회 호출 비용\n",
    "\n",
    "- Claude 3 Opus $24,555 / 33,885,900원\n",
    "- Claude 3 Sonnet $4,911 / 6,777,180원\n",
    "- Claude 3 Haiku $409.25 / 564,765원\n",
    "- Gemini Pro $3,439.13 / 4,746,002원\n",
    "- Gemini Flash $343.91 / 474,600원\n",
    "- GPT-4o $4,281.92 / 5,909,054원\n",
    "- GPT-4 $12,078.71 / 16,668,624원\n",
    "- GPT-3.5 $603.94 / 833,431원\n",
    "\n",
    "#### 5000만회 호출 비용\n",
    "\n",
    "- Claude 3 Opus $1,227,750 / 1,694,295,000원\n",
    "- Claude 3 Sonnet $245,550 / 338,859,000원\n",
    "- Claude 3 Haiku $20,462.50 / 28,238,250원\n",
    "- Gemini Pro $171,956.59 / 237,300,099원\n",
    "- Gemini Flash $17,195.66 / 23,730,010원\n",
    "- GPT-4o $214,096.15 / 295,452,692원\n",
    "- GPT-4 $603,935.64 / 833,431,177원\n",
    "- GPT-3.5 $30,196.78 / 41,671,559원"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kakao",
   "language": "python",
   "name": "kakao"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
